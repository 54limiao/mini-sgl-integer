- add w8a8 compressed tensor model load support
- add quantized i32 rmsnorm and residual
- add quantized i32 mlp
- add quantized i32 atten triton
- add to flash atten
- modify silu
- modify rope to rope hadamard
- modify r4 support


          integer    w8a8     float
gms8k     0.8469     0.8506   0.8916

evalscope eval \
 --model "Qwen3-8B" \
 --api-url http://127.0.0.1:1919/v1 \
 --api-key EMPTY \
 --eval-type openai_api \
 --generation-config '{"do_sample":false,"temperature":0.0,"max_tokens":2048}' \
 --eval-batch-size 128 \
 --datasets gsm8k


/workspace/lim42@xiaopeng.com/github/quant_bench/data/models/Qwen/Qwen3-8B-r1r2-gptq-w8a8-dynamic

uv sync -i https://pypi.tuna.tsinghua.edu.cn/simple

# basic
CUDA_VISIBLE_DEVICES=0 python -m minisgl --model "/workspace/lim42@xiaopeng.com/github/quant_bench/data/models/Qwen/Qwen3-8B" --shell
# wa8a8
CUDA_VISIBLE_DEVICES=0 python -m minisgl --model "/workspace/lim42@xiaopeng.com/github/quant_bench/data/models/Qwen/Qwen3-8B-r1r2-gptq-w8a8-dynamic" --shell
# integer-only
MINISGL_INTEGER_MODE=1 MINISGL_MAX_INT_LAYERS=-1  CUDA_VISIBLE_DEVICES=0 python -m minisgl --model "/workspace/lim42@xiaopeng.com/github/quant_bench/data/models/Qwen/Qwen3-8B-r1r2-gptq-w8a8-dynamic" --shell

MINISGL_INTEGER_ROPE=1 MINISGL_INTEGER_MLP=1 MINISGL_INTEGER_MODE=1 MINISGL_MAX_INT_LAYERS=-1  CUDA_VISIBLE_DEVICES=0 python -m minisgl --model "/workspace/lim42@xiaopeng.com/github/quant_bench/data/models/Qwen/Qwen3-8B-r1r2-gptq-w8a8-dynamic" --port 1919 --shell


